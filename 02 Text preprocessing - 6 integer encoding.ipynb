{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy\n",
    "import numpy as np\n",
    "# pip install pandas\n",
    "import pandas as pd\n",
    "# dplython\n",
    "import pandas as pd\n",
    "from dplython import (DplyFrame, X, diamonds, select, sift,\n",
    "  sample_n, sample_frac, head, arrange, mutate, group_by,\n",
    "  summarize, DelayFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b51fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"D:/대학원/논문/커뮤니케이션학과/reply_paste.txt\",\"r\")\n",
    "reply_text = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(reply_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b87776",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076618fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "# 기본 딕셔러니 -> Key1:Value1\n",
    "# items :쌍\n",
    "# item[0]은 dict의 key, item[1]은 dict의 value\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 부여\n",
    "word_to_index = {}\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a47e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, freq) in vocab_sorted :\n",
    "    if freq > 2 : # 빈도수가 작은 단어는 제외.\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcff237",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "# 인덱스가 5 초과인 단어 제거\n",
    "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
    "\n",
    "# 해당 단어에 대한 인덱스 정보를 삭제\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "            encoded_sentence.append(word_to_index[\"OOV\"])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data frame\n",
    "# sort\n",
    "vocab_df = pd.DataFrame.from_dict(data = vocab, orient=\"index\")\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = DplyFrame(vocab_df)\n",
    "vocab_df.columns = [\"Freq\"]\n",
    "\n",
    "vocab_df = vocab_df.sort_values(\"Freq\", ascending = False) >> sift(X.Freq > 2)\n",
    "vocab_df[\"word\"] = vocab_df.index.to_list()\n",
    "vocab_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encoding\n",
    "integer = []\n",
    "for i in range(1,len(vocab_df)+1):\n",
    "    integer.append(i)\n",
    "    \n",
    "vocab_df[\"encoding\"] = integer\n",
    "vocab_df = vocab_df >> sift(X.encoding <= 5) >> select(X.word,X.encoding)\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'word' : ['OOV'], 'encoding' : ['6']})\n",
    "vocab_df = pd.concat([vocab_df,df2], ignore_index = False)\n",
    "vocab_df.set_index(\"word\", inplace = True)\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = vocab_df.to_dict(\"dict\")\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index = vocab_dict[\"encoding\"]\n",
    "print(vocab_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "            encoded_sentence.append(vocab_index[word])\n",
    "        except KeyError:\n",
    "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "            encoded_sentence.append(vocab_index[\"OOV\"])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91307db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스(Keras)의 텍스트 전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b90b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpreprocessed_sentences\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(preprocessed_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd24547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환합니다.\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02305ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1cdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78428ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
